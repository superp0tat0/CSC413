{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biggan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGxmX6RHxrxY"
      },
      "source": [
        "# BigGAN\n",
        "For this part, we will implement the interpolation function that you see in many GAN papers to show that the GAN model can generate novel images between classes. We will use BigGAN as our learned model to do the interpolation.\n",
        "\n",
        "BigGAN [1] is an approach to pull together a suite of recent best practices in training GANs and scaling up the model to large-scale datasets like Imagenet. The result is the routine generation of both high-resolution (large) and high-quality (high-fidelity) images.\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/qLVZrbJ.png)\n",
        "Figure 1: Class-conditional samples generated by BigGAN. (Figure screenshot from [1])\n",
        "\n",
        "\n",
        "[1] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. arxiv:1809.11096, 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeiP8aG9yGT1"
      },
      "source": [
        "\n",
        "\n",
        "__BigGAN Tricks__\n",
        "\n",
        "BigGAN benefits from larger batch size and more expressive generator and discriminator. A varied range of techniques had been proposed before BigGAN to improve GAN in terms of samples quality, diversity and training stability.BigGAN utilized [self-attention module](https://arxiv.org/abs/1805.08318), [moving average `G` weights](https://arxiv.org/abs/1710.10196) , [orthogonal initialization](https://arxiv.org/abs/1312.6120), and different residual block structure. See figure 15(a) in [1] for the illustration of the residual block for `G`. They also proposed trucnation of noise distribution to control fidelity vs quality of samples. The paper shows that applying [Spectral Normalization](https://arxiv.org/abs/1802.05957) in `G` improves stability, allowing for fewer `D` steps per iteration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSVBE4RLm73i"
      },
      "source": [
        "!rm -rf BigGAN-Helpers\n",
        "!git clone https://github.com/sajadn/BigGAN-Helper.git\n",
        "!wget www.cs.toronto.edu/~sajadn/biggan-256.pth -P BigGAN-Helper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2P9OM4Boy4f"
      },
      "source": [
        "pip install parse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJIABub0ob-H"
      },
      "source": [
        "## Load Pre-trained BigGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMDwC2s7p-yy"
      },
      "source": [
        "def imshow(img):\n",
        "  img = torchvision.utils.make_grid(img.reshape(-1, 3, 256, 256))\n",
        "  plt.rcParams['figure.figsize'] = 20, 10\n",
        "  plt.axis(\"off\")\n",
        "  npimg = img.detach().cpu().numpy()\n",
        "  npimg -= npimg.min()\n",
        "  npimg /= npimg.max()\n",
        "  plt.imshow((np.transpose(npimg, (1, 2, 0))))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKxhfgtTYV28"
      },
      "source": [
        "#Loading pretrained generator\n",
        "%run BigGAN-Helper/BigGAN.py\n",
        "DEVICE = 'cuda'\n",
        "config = get_config()\n",
        "G = Generator(**config)\n",
        "state_dict_path = 'BigGAN-Helper/biggan-256.pth'\n",
        "G.load_state_dict(torch.load(state_dict_path), strict=False) # Ignore missing sv0 entries\n",
        "G.to(DEVICE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0tqZa0olO5"
      },
      "source": [
        "## Visualize Class Embeddings using T-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWrp34qdg6TU"
      },
      "source": [
        "#Collecting class embeddings\n",
        "class_emb_list = []\n",
        "for i in range(1000):\n",
        "  #G.shared receives a class label and returns the corresponding embeding\n",
        "  emb_vector = G.shared(torch.tensor(i).to(DEVICE)).cpu()\n",
        "  class_emb_list.append((emb_vector).detach().numpy())\n",
        "class_emb_vectors = np.array(class_emb_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1DQ2Ts3qsQG"
      },
      "source": [
        "#T-SNE on class embedings\n",
        "from sklearn.manifold import TSNE\n",
        "class_tsne = TSNE(n_components=2).fit_transform(class_emb_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9BKK0fuqyEr"
      },
      "source": [
        "#Collecting label of each class\n",
        "!wget https://gist.githubusercontent.com/andrewliao11/79f48f77bda4d0c9efdd589a5fbabfd5/raw/83b08f576586bf6589cdf21341cf71a528e56756/imagenet_id_to_label.json\n",
        "import json\n",
        "with open('imagenet_id_to_label.json') as f:\n",
        "    imagenet_id_to_label = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clt5L7rQqsEi"
      },
      "source": [
        "index = np.random.choice(np.arange(1000), size=100, replace=False)\n",
        "z = class_tsne[index,0]\n",
        "y = class_tsne[index,1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), facecolor='white')\n",
        "plt.scatter(z, y, s=50, c='c')\n",
        "for i, id in enumerate(index):\n",
        "    ax.annotate(imagenet_id_to_label[str(id)], (z[i], y[i]), fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHfxccN2o1JL"
      },
      "source": [
        "# Sample from BigGAN from a Given Class ID\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKNZY537gOe7"
      },
      "source": [
        "#To sample from generator\n",
        "def generate_sample(G, batch_size, class_label):\n",
        "  G.eval()\n",
        "  with torch.no_grad():\n",
        "    z = torch.randn(batch_size, G.dim_z).to(DEVICE)\n",
        "    y = torch.tensor(class_label).to(DEVICE)*torch.ones((batch_size,)).to(DEVICE).long()\n",
        "    y_emb = G.shared(y)\n",
        "    images = G(z, y_emb)\n",
        "  return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZXEFkv1razP"
      },
      "source": [
        "batch_size = 1\n",
        "class_label = 2\n",
        "img_list = []\n",
        "for i in range(16):\n",
        "  img = generate_sample(G, batch_size, class_label)\n",
        "  img_list.append(img)\n",
        "img_list = torch.cat(img_list, dim=0)\n",
        "imshow(img_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96P374UzpDSd"
      },
      "source": [
        "# Linear Interpolation Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WshxxOx6kZBI"
      },
      "source": [
        "#Linear interpolation between two class embedings\n",
        "def generate_linear_interpolate_sample(G, batch_size, class_label1, class_label2, alpha):\n",
        "  G.eval()\n",
        "  G.to(DEVICE)\n",
        "  with torch.no_grad():\n",
        "    z = torch.randn(batch_size, G.dim_z).to(DEVICE)\n",
        "    class1_emb = G.shared(torch.tensor(class_label1).to(DEVICE)*torch.ones((batch_size,)).to(DEVICE).long())\n",
        "    class2_emb = G.shared(torch.tensor(class_label2).to(DEVICE)*torch.ones((batch_size,)).to(DEVICE).long())\n",
        "\n",
        "    ###########################################\n",
        "    ##   FILL THIS IN: CREATE NEW EMBEDDING  ##\n",
        "    ###########################################    \n",
        "    # new_emb = ...\n",
        "\n",
        "    images = G(z, new_emb)\n",
        "  return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEtl2ijYWMM-"
      },
      "source": [
        "batch_size = 1\n",
        "class_label_1 = 1\n",
        "class_label_2 = 2\n",
        "\n",
        "img_list = []\n",
        "n = 16\n",
        "for i in range(n):\n",
        "  img = generate_linear_interpolate_sample(G, batch_size, class_label_1, class_label_2, i*(1/n))\n",
        "  img_list.append(img)\n",
        "img_list = torch.cat(img_list, dim=0)\n",
        "imshow(img_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv1wqQFwqf7e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}